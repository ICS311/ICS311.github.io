<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title>ICS 311 #22:Multithreaded Algorithms </title>
</head>

<body>

<hr><h1><a href="../index.html">ICS 311</a> #22: Multithreaded Algorithms </h1><hr> 

<!-- ------------------------------------------------------------ -->
<h2>Outline</h2> 
<ol>
  <li>Concepts of Dynamic Multithreading </li>
  <li>Modeling Dynamic Multithreading </li>
  <li>Measuring Dynamic Multithreading </li>
  <li>Analysis of Multithreaded Algorithms </li>
  <li>Example: Matrix Multiplication</li>
  <li>Example: Merge Sort</li>
  <li>Scheduling</li> 
</ol>
  
<h2>Readings</h2>
<ul>
  <li>CLRS 3rd Ed. Chapter 27.</li>
  <li>Screencasts not available. (This topic was added later, as a "special
  topic".) Read this document carefully.</li>
</ul> 

<!-- ------------------------------------------------------------ -->
<hr><h2> Concepts of Dynamic Multithreading </h2>

<p>Multithreading is a crucial topic for modern computing. Parallel machines are
getting cheaper and in fact are now ubiquitous ...</p> 
<ul> 
  <li>supercomputers: custom architectures and networks</li>
  <li>computer clusters with dedicated networks (distributed memory)</li>
  <li>multi-core integrated circuit chips (shared memory)</li>
  <li>GPUs (graphics processing units) with multiple processors </li> 
</ul>

<p> Our emphasis here will be <b>parallel algorithms</b>, that is,
multithreading a single algorithm so that some of its instructions may be
executed simultaneously. Parallelism can also be applied to scheduling and
managing multiple algorithms, each running concurrently in their own thread and
possibly sharing resources, as studied in courses on operating systems and
concurrent and high performance computing.</p>

<!-- -------------------------------- -->
<h3>Static and Dynamic Multithreading</h3>

<p><b>Static threading</b> provides the programmer with an abstraction of
virtual processors that are managed explicitly. It's "static" because the
programmer must specify in advance how many processors to use at each
point. This can be difficult and inflexible with respect to evolving
conditions.</p>

<p>Rather than managing threads explicitly, our model is <b>dynamic
multithreading</b> in which programmers specify opportunities for parallelism,
and a <b>concurrency platform</b> manages the decisions of mapping these
opportunities to actual static threads.</p>

<!-- -------------------------------- -->
<h3>Concurrency Constructs:</h3>

<p>We will use three keywords in our pseudocode, reflecting current
parallel-computing practice:</p>

<ul>

  <li><b>parallel</b>: add to loop construct such as <b><tt> for</tt></b> to
      indicate each iteration can be executed in parallel.</li><br>
  
  <li><b>spawn</b>: create a parallel subprocess, then keep executing the
      current process (parallel procedure call).</li><br>
      
  <li><b>sync</b>: wait here until all active parallel threads created by this
      instance of the program finish; used when one cannot proceed without
      pending results.</li>
      
</ul>

<p>These keywords specify opportunities for parallelism without affecting
whether (or not) the corresponding sequential program obtained by removing them
is correct. In other words, if we ignore the parallel keywords the program can
be analyzed as a single threaded program. We exploit this in analysis. </p>

<!-- -------------------------------- -->
<h3>Logical Parallelism</h3>

<p>The <b> parallel</b> and <b>spawn</b> keywords do not force parallelism: they
just says that it is permissible. This is <b>logical parallelism</b>. A
scheduler will make the decision concerning allocation to processors. We return
to the question of scheduling at the end of this document, after approriate
concepts have been introduced.</p>

<p>However, if parallelism is used, <b>sync</b> must be respected. For safety,
<u> there is an implicit sync at the end of every procedure</u>.</p>

<!-- -------------------------------- -->
<h3>Example: Parallel Fibonacci</h3> 

<p>For illustration, we take a really slow algorithm and make it
parallel. (There are much better ways to compute Fibonacci numbers using dynamic
programming; this is just for illustration.) Here is the definition of Fibonacci
numbers:</p>

<blockquote>
F<sub>0</sub> = 0.<br>
F<sub>1</sub> = 1.<br>
F<sub><i>i</i></sub> = F<sub><i>i-1</i></sub> + F<sub><i>i-2</i></sub>, for <i>i</i> &ge; 2.
</blockquote>

<p>Here is a recursive non-parallel algorithm for computing Fibonacci numbers
modeled on the above definition, along with its recursion tree:</p>
<img src="Topic-22/code-Fib.jpg">
<img src="Topic-22/Fig-27-1-Fib-Recursion-Tree-Small.jpg" align ="right">

<p><b><tt>Fib</tt></b> has recurrence relation T(<i>n</i>) = T(<i>n</i> &minus;
1) + T(<i>n</i> &minus; 2) + &Theta;(1), which has the solution T(<i>n</i>) =
&Theta;(F<sub><i>n</i></sub>) (see the text for substitution method proof). This
grows exponentially in <i>n</i>, so it's not very efficient. (A straightforward
iterative algorithm is much better.) </p>

<p>Noticing that the recursive calls operate independently of each other, let's
see what improvement we can get by computing the two recursive calls in
parallel. This will illustrate the concurrency keywords and also be an example
for analysis:</p>

<img src="Topic-22/code-P-Fib.jpg">

<p>Notice that without the parallel keywords it is the same as the serial
program above.</p>

<p>We will return to this example when we analyze multithreading.</p>

<!-- ------------------------------------------------------------ -->
<hr><h2> Modeling Dynamic Multithreading </h2>

<p>First we need a formal model to describe parallel computations.</p>

<!-- -------------------------------- -->
<h3>A Model of Multithreaded Execution</h3>

<p>We will model a multithreaded computation as a <b>computation DAG</b>
(directed acyclic graph) <i>G</i> = (<i>V</i>, <i>E</i>):

<ul>
  
  <li> <u>Vertices</u> in <i>V</i> represent instructions. To simplify the
      graph, each vertex can represent a <b>strand</b>: a sequence of
      non-parallel instructions, as they will all be treated the same as far as
      parallelism is concerned.</li> <br>

  <li><u>Edges</u> in <i>E</i> represent <b>dependencies</b> between
      instructions or strands: (<i>u</i>, <i>v</i>) &in; <i>E</i> means <i>u</i>
      must execute before <i>v</i>. (Edge are categorized in ways elaborated
      below.)</li><br>

  <li>If <i>G</i> has a <u>directed path</u> from <i>u</i> to <i>v</i> they are
      logically in <b>series</b>; otherwise they are logically
      <b>parallel</b>.</li><br>

  <li>A strand or vertex with <u>multiple successors</u> means all but one of the successors had to be spawned. A strand with </u>multiple predecessors</u> means the predecessors are joining
      at a sync statement.</li>

</ul> 

<p>We assume an ideal parallel computer with <b>sequentially consistent</b>
memory, meaning it behaves as if the instructions were executed sequentially in
some full ordering consistent with orderings within each thread (i.e.,
consistent with the partial ordering of the computation DAG). </p>
   
<h3>Visualizing the Model</h3>

<p>The model can be visualized as exemplified below for the computation DAG for P-Fib(4):</p> 

<img src="Topic-22/code-P-Fib.jpg" align="right">

<img src="Topic-22/Fig-27-2-P-Fib.jpg">

<p>Vertices (strands) are visualized as circles in the figure.</p> 
<ul>
  
  <li> The rounded rectangles are not part of the formal model, but they help
      organize the visualization by collecting together all strands for a given
      call. </li>
  
  <li> The colors are specific to this example and indicate the corresponding
      code: black indicates that the strand is for lines 1-3; grey for line 4;
      and white for lines 5-6.</li>
  
</ul> 

<p>Edges are categorized and visualized as follows:</p> 
<ul>
  
  <li><b>Continuation Edges</b> (<i>u</i>, <i>v</i>) are drawn horizontally and
      indicate that <i>v</i> is the successor to <i>u</i> in the sequential
      procedure.</li>
  
  <li><b>Call Edges</b> (<i>u</i>, <i>v</i>) point downwards, indicating that
      <i>u</i> called <i>v</i> as a normal subprocedure call. In this example
      they come out of the grey circles.</li>
  
  <li><b>Spawn Edges</b> (<i>u</i>, <i>v</i>) also point downwards, indicating
      that <i>u</i> spawned <i>v</i> in parallel. In this example they come out
      of the black circles.</li>
  
  <li><b>Return edges</b> point upwards to indicate the next strand executed
      after returning from a normal procedure call, or after parallel spawning
      at a sync point. In this example they return to the white circles.</li>
  
</ul>

<!-- ------------------------------------------------------------ -->
<hr><h2> Measuring Dynamic Multithreading </h2>

<p>We write <b><i>T</i><sub><i>P</i></sub></b> to indicate the <u>running time
of an algorithm on <i>P</i> processors</u>. Then we define these measures and
laws: </p>

<!-- -------------------------------- -->
<h3>Work</h3> 

<p><b><i>T</i><sub>1</sub></b> = the total time to execute an algorithm on one
processor. This is called <i>work</i> analogous to work in physics: the total
amount of computational work that gets done.</p>

<p>An ideal parallel computer with <i>P</i> processors can do at most <i>P</i>
units of work in one time step. So, in <i>T</i><sub><i>P</i></sub> time it can
do at most <i>P</i>&sdot;<i>T</i><sub><i>P</i></sub> work. Since the total work
is <i>T</i><sub>1</sub>, &nbsp; <i>P</i>&sdot;<i>T</i><sub><i>P</i></sub> &ge;
<i>T</i><sub>1</sub>, &nbsp; or dividing by P we get the <b>work law:</b></p>

<blockquote>
   <b><i>T</i><sub><i>P</i></sub>&nbsp; &ge; &nbsp; <i>T</i><sub>1</sub> /
   <i>P</i></b> 
</blockquote>

<p>The work law can be read as saying that the speedup for <i>P</i> processors
can be no better than the time with one processor divided by <i>P</i>. That
is,</p>

<blockquote>
  <b>parallelism on P processors at best gives constant
  speedup where the constant is 1/<i>P</i></b>.
</blockquote>

<p><b>Parallelism will not change the asymptotic class of an algorithm</b>: it's
not a substitute for careful design of asymptotically fast algorithms.</p>

<!-- -------------------------------- -->
<h3>Span</h3>

<p>The <b>span</b> of a multithreaded computation is the <u>longest time to
execute the strands along any path in the computation DAG</u>. If each strand
(represented by vertices) takes a unit of time, then this will be the number of
vertices on the longest path in the DAG, which we call the <b>critical
path</b>. If strands take different amounts of time then the critical path will
be the path with the greatest cost, summing the costs associated with the
vertices. </p>

<img src="Topic-22/Fig-27-2-P-Fib.jpg" align = "right">

<p>(Readers in our classes may recall the class excercise on finding the
shortest time you can complete a set of interdependent jobs by finding the
longest path in the job DAG: the concept here is similar.)</p>

<p>We can also define span as <b><i>T</i><sub>&infin;</sub></b> = the total time
to execute an algorithm on an infinite number of processors &mdash; or, more
practically speaking, on just as many processors as are needed to allow
parallelism wherever it is possible. It is the fastest we can
possibly expect &mdash; an &Omega; bound -- because no matter how many
processors you have, the algorithm must take this long.</p>

<p>The critical path in our P-Fib example is represented by the shaded edges in
the figure. Notice that span is <u>not</u> simply the costs on the path from the
root to the leaves of the recursion tree: Once the recursion has hit the base
case the execution still needs to proceed as the recursion unwinds.</p>

<p>The <b>span law</b> states that a <i>P</i>-processor ideal parallel computer
cannot run faster than one with an infinite number of processors:</p>

<blockquote>
<b><i>T</i><sub><i>P</i></sub> &nbsp; &ge; &nbsp; <i>T</i><sub>&infin;</sub></b>
</blockquote>

<p>This is because at some point the span will limit the speedup possible: No
matter how many processors you have, you still must do these strands in
sequence, taking the time they require.</p>

<p><i><u>Exercise</u>: If we count each vertex as one unit of work, <a href =
"Topic-22/exercise-work-span.html">what is the work and span of the computation
DAG for P-Fib shown?</a></i> </p>

<!-- -------------------------------- -->
<h3>Speedup</h3>

<p>The ratio <b><i>T</i><sub>1</sub> / <i>T</i><sub><i>P</i></sub></b> defines
how much <b>speedup</b> you get with <i>P</i> processors as compared to one.</p>

<p>By the work law,</p>

<blockquote>
  <i>T</i><sub><i>P</i></sub> &nbsp; &ge; &nbsp; <i>T</i><sub>1</sub> /
  <i>P</i>, &nbsp; &nbsp; so &nbsp; &nbsp; <i>T</i><sub>1</sub> /
  <i>T</i><sub><i>P</i></sub> &nbsp; &le; &nbsp; <i>P</i>:
</blockquote>

<p>one cannot have any more speedup than the number of processors.</p>

<p>This is important enough to repeat: <b>parallelism provides only constant
time improvements</b> (the constant being the number of processors) to any
algorithm! <b>Parallelism cannot move an algorithm from a higher to lower
complexity class</b> (e.g., exponential to polynomial, or quadratic to
linear). Parallelism is not a silver bullet: good algorithm design and analysis
is still needed. </p>

<p>When the speedup <i>T</i><sub>1</sub> / <i>T</i><sub><i>P</i></sub> =
&Theta;(<i>P</i>) we have <b>linear speedup</b>: the speedup is linear in the
number of processors.</p>

<p>When <i>T</i><sub>1</sub> / <i>T</i><sub><i>P</i></sub> = <i>P</i> we have
<b>perfect linear speedup</b>: we got the maximum amount of speedup possible
from each processor. </p>

<!-- -------------------------------- -->
<h3>Parallelism</h3>

<p>The ratio <b><i>T</i><sub>1</sub> / <i>T</i><sub>&infin;</sub></b> of the
work to the span gives the (potential) <b>parallelism</b> of the computation. It
can be interpreted in three ways:</p>

<ul>
  
  <li><i>Ratio </i>: The average amount of work that can be performed for each
      step of parallel execution time.</li> <br>
  
  <li><i>Upper Bound </i>: the maximum possible speedup that can be achieved on
      any number of processors.</li> <br>
  
  <li><i>Limit</i>: The limit on the possibility of attaining perfect linear
      speedup. Once the number of processors exceeds the parallelism, the
      computation cannot possibly achieve perfect linear speedup. The more
      processors we use beyond parallelism, the less perfect the speedup.</li>
  
</ul>

<p>This latter way of looking at <i>T</i><sub>1</sub> /
<i>T</i><sub>&infin;</sub> leads to the concept of <b>parallel
slackness</b>:</p>

<blockquote>
  <b>(<i>T</i><sub>1</sub> / <i>T</i><sub>&infin;</sub>) / <i>P</i>&nbsp; =
  &nbsp; <i>T</i><sub>1</sub> / (<i>P</i>&sdot;<i>T</i><sub>&infin;</sub>)</b>,
</blockquote>

<p> the factor by which the parallelism of the computation exceeds the number of
processors in the machine. We have three cases:</p>

<ul>
  
  <li> If slackness is less than 1 then perfect linear speedup is not possible:
      you have more processors than you can make use of. </li>

  <li> If slackness is greater than 1, then the work per processor is the
      limiting constraint and a scheduler can strive for linear speedup by
      distributing the work across more processors.</li>

  <li> If slackness is 1, (<i>T</i><sub>1</sub> / <i>T</i><sub>&infin;</sub>) /
      <i>P</i> = 1 so <i>T</i><sub>1</sub> / <i>T</i><sub>&infin;</sub> =
      <i>P</i>: we get perfect linear speedup with <i>P</i> processors. </li>

</ul>

<p><i><u>Exercise:</u> <a href="Topic-22/exercise-parallelism.html">What is the
parallelism of the computation DAG for P-Fib shown previously? What does this
parallelism say about the prospects for speedup at *this* n? </a> What happens
to work and span as n grows?</i> </p>

<!-- ------------------------------------------------------------ -->
<hr><h2>Analysis of Multithreaded Algorithms</h2>

<p>Analyzing <i>work</i> is simple: ignore the parallel constructs and analyze
the serial algorithm.</p>

<p>For example, we already noted previously that the work of
<tt>P-Fib(<i>n</i>)</tt> is</p> <blockquote> T(<i>n</i>) = T(<i>n</i> &minus; 1)
+ T(<i>n</i> &minus; 2)+ &Theta;(1), </blockquote> <p>which has the solution
T(<i>n</i>) = &Theta;(F<sub><i>n</i></sub>), the <u>work</u> of
<tt>P-Fib(<i>n</i>)</tt>.</p>

<p>Analyzing <i>span</i> requires a different approach. (I hope you did the
exercises above: they will make you appreciate the following all the more.)</p>

<!-- -------------------------------- -->
<h3>Analyzing Span</h3> 

<p>If a set of subcomputations (or the vertices representing them) are in
<u>series</u>, the span is the <u>sum</u> of the spans of the
subcomputations. This is like normal sequential analysis (as was just
exemplified above with the sum T(<i>n</i> &minus; 1) + T(<i>n</i> &minus;
2)). </p>

<p>If a set of subcomputations (or the vertices representing them) are in
<u>parallel</u>, the span is the <u>maximum</u> of the spans of the
computations. This is where analysis of multithreaded algorithms differs. </p>

<img src="Topic-22/Fig-27-3-Composed-Subcomputations.jpg">

<p>Returning to our example, the span of the parallel recursive calls of
<tt>P-Fib(<i>n</i>)</tt> is computed by taking the max rather than the sum:</p>

<blockquote>
  <i>T</i><sub>&infin;</sub> (<i>n</i>) &nbsp; = &nbsp;
    max(T</i><sub>&infin;</sub>(<i>n</i>&minus;1), T</i><sub>&infin;</sub>
    (<i>n</i>&minus;2)) + &Theta;(1) <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
    &nbsp; &nbsp; = &nbsp;T</i><sub>&infin;</sub>(<i>n</i>&minus;1) +
    &Theta;(1).
</blockquote>

<p>The recurrence T</i><sub>&infin;</sub> (<i>n</i>) =
T</i><sub>&infin;</sub>(<i>n</i>&minus;1) + &Theta;(1) has solution
&Theta;(<i>n</i>). So the <u>span</u> of <tt>P-Fib(<i>n</i>)</tt> is
&Theta;(<i>n</i>).</p>

<p>We can now compute the <u>parallelism</u> of <tt>P-Fib(<i>n</i>)</tt> in
general (not just the specific case of <i>n</i>=4 that we computed earlier) by
dividing its work &Theta;(F<sub><i>n</i></sub>) by its span &Theta;(<i>n</i>):
</p>

<blockquote>
  T<sub>1</sub>(<i>n</i>) / T<sub>&infin;</sub> &nbsp; = &nbsp;
  &Theta;(F<sub><i>n</i></sub>) / &Theta;(<i>n</i>) &nbsp; = &nbsp;
  &Theta;(F<sub><i>n</i></sub>/<i>n</i>)
</blockquote>

This grows dramatically, as F<sub><i>n</i></sub> grows much faster than
<i>n</i>. </p>

<p>For any given number of processors <i>P</i>, there is considerable parallel
slackness &Theta;(F<sub><i>n</i></sub>/<i>n</i>)/<i>P</i>. For any <i>P</i>
above some <i>n</i> there is likely to be something for additional processors to
do. Thus there is potential for near perfect linear speedup as <i>n</i>
grows.</p>

<p><i>Of course in this example it's because we chose an inefficent way to
compute Fibonacci numbers, but this was only for illustration. These ideas apply
to other well designed algorithms.</i></p>

<!-- -------------------------------- -->
<h3>Parallel Loops</h3> 

<p>So far we have used spawn, but not the <b>parallel</b> keyword, which is used
with loop constructs such as <b>for</b>. Here is an example. </p>

<p>Suppose we want to multiply an <i>n</i> x <i>n</i> matrix <i>A</i> =
(<i>a</i><sub><i>ij</i></sub>) by an <i>n</i>-vector <i>x</i> =
(<i>x</i><sub><i>j</i></sub>). This yields an <i>n</i>-vector <i>y</i> =
(<i>y</i><sub><i>i</i></sub>) where:</p>

<img src="Topic-22/equation-matrix-vector-mult.jpg">

<p>The following algorithm does this in parallel:</p> <img
src="Topic-22/code-Mat-Vec.jpg"> <p>The <b>parallel for</b> keywords indicate
that each iteration of the loop can be executed concurrently. (Notice that the
inner <b>for</b> loop is not parallel; a possible point of improvement to be
discussed.)</p>

<h4>Implementing Parallel Loops</h4> 

<p>It is not realistic to think that all <i>n</i> subcomputations in these loops
(lines 3 and 5) can be spawned simultaneously with no extra work. <i>(For some
operations on some hardware up to a constant <i>n</i> this may be possible;
e.g., hardware designed for matrix operations; but we are concerned with the
general case.)</i> How might this parallel spawning be done, and how does this
affect the analysis?</p>

<p><b>Parallel for</b> spawning can be accomplished by a compiler with a
divide and conquer approach, itself implemented with parallelism!</b>

<p>Consider how to implement parallelism in lines 5-7 of <tt>Mat-Vec</tt>
above. The concurrency platform compiler can arrange for the procedure shown
below to be called instead, <tt>Mat-Vec-Main-Loop</tt>(<i>A</i>, <i>x</i>,
<i>y</i>, <i>n</i>, 1, <i>n</i>). Lines 2 and 3 below are the lines originally
within the loop.</p>

<img src="Topic-22/code-Mat-Vec-Main-Loop.jpg">
<img src="Topic-22/Fig-27-4-Mat-Vec-Main-Loop-small.jpg" align="right">

<p>The computation DAG is also shown. It appears that a lot of work is being
done to spawn the <i>n</i> leaf node computations, but the increase is not
asymptotically significant.</p>

<p>The <i>work</i> of <tt>Mat-Vec</tt> is T<sub>1</sub>(<i>n</i>) =
&Theta;(<i>n</i><sup>2</sup>) due to the nested loops in 5-7.</p>

<p>Since the computation DAG is a full binary tree, the number of internal nodes
is 1 fewer than the <i>n</i> leaf nodes (<a
href="Topic-08.html#binarytrees">Topic 8</a>), so this extra work is
&Theta;(<i>n</i>).</p>

<p>Each leaf node corresponds to one iteration of loop, and the extra work of
recursive spawning can be amortized across the work of the iterations, so that
it contributes only constant work.</p>

<p>Concurrency platforms sometimes coarsen the recursion tree by executing
several iterations in each leaf, reducing the amount of recursive spawning.</p>

<p>The <i>span</i> is increased by &Theta;(lg <i>n</i>) due to the addition of
the recursion tree for <tt>Mat-Vec-Main-Loop</tt>, which is of height &Theta;(lg
<i>n</i>). In some cases (such as this one), this increase is washed out by
other dominating factors (e.g., the span in this example is dominated by the
nested loops). </p>

<h4>Nested Parallelism</h4>

<p>Continuing with our example, the span is &Theta;(<i>n</i>) because even with
full utilization of parallelism the inner <b>for</b> loop still requires
&Theta;(<i>n</i>). Since the work is &Theta;(<i>n</i><sup>2</sup>) the
parallelism is &Theta;(<i>n</i><sup>2</sup>)/&Theta;(<i>n</i>) =
&Theta;(<i>n</i>). Can we improve on this?</p>

<p>Perhaps we could make the inner <b>for</b> loop parallel as well? Compare the
original to the revised version <tt>Mat-Vec'</tt>:</p>

<img src="Topic-22/code-Mat-Vec.jpg">
<img src="Topic-22/code-Mat-Vec-Prime.jpg" align = "right">

<p>Would it work? We need to introduce a new concept ...</p> 

<!-- -------------------------------- -->
<h3>Race Conditions</h3>

<p><b>Deterministic</b> algorithms do the same thing on the same input; while
<b> nondeterministic</b> algorithms may give different results on different
runs.</p>

<p>The above Mat-Vec' algorithm is subject to a potential problem called a
<b>determinancy race</b>: when the outcome of a computation could be
nondeterministic (unpredictable). In general, <u>this can only happen when two
logically parallel computations access the same memory and one performs a
write</u>. </p>

<p>Determinancy races are hard to detect with empirical testing: many execution
sequences would give correct results. <u>This kind of software bug is
consequential</u>: Race condition bugs caused the Therac-25 radiation machine to
overdose patients, <u>killing three</u>; and caused the North American Blackout
of 2003.</p>

<img src="Topic-22/Fig-27-5-Race-Condition.jpg" align="right"> 

<p>For example, the code shown below might output 1 or 2 depending on the order
in which access to <i>x</i> is interleaved by the two threads:</p>

<img src="Topic-22/code-Race-Example.jpg">

<br>

<p> The value of <i>x</i> must first be read into a register <i>r</i> before it
is operated on. In this case, there are two registers. It is incremented in the
register and then written back out to memory. The table indicates one possible
computation sequence that gives the unexpected result. </p>

<p>After you understand that simple example, let's look at our (renamed)
matrix-vector example again:</p>

<img src="Topic-22/code-Mat-Vec-Wrong.jpg">

<p><i><u>Exercise</u>: Do you see how <i>y<sub>i</sub></i> <a
 href="Topic-22/exercise-race-condition-mat-vec.html">might be updated
 differently</a> depending on the order in which parallel invocations of line 7
 (including access to current value of y<sub>i</sub> and writing new ones) are
 executed?</i> Keep in mind that race conditions can only happen when two
 logically parallel computations access the same memory and one performs a
 write.</p>

<!-- ------------------------------------------------------------ -->
<hr><h2> Example: Matrix Multiplication </h2>

<h4> Multithreading the basic algorithm</h4>

<p>Here is an algorithm for multithreaded matrix multiplication, based on the
T<sub>1</sub>(<i>n</i>) = &Theta;(<i>n</i><sup>3</sup>) algorithm:</p> <img
src="Topic-22/code-P-Square-Matrix-Multiply.jpg">

<p><i><u>Exercise</u>: How does this procedure compare to MAT-VEC-WRONG? Both of
them have nested <tt>parallel for</tt> loops: <a
href="Topic-22/exercise-race-condition-mat-mult.html"> Is
P-SQUARE-MATRIX-MULTIPLY also subject to a race condition? Why or why not?</a>
</i></p>

<p>The span of this algorithm is T<sub>&infin;</sub>(<i>n</i>) &nbsp; = &nbsp;
&Theta;(<i>n</i>), due to the path for spawning the outer and inner parallel
loop executions and then the <i>n</i> executions of the innermost <b>for</b>
loop. So the parallelism is T<sub>1</sub>(<i>n</i>) /
T<sub>&infin;</sub>(<i>n</i>) &nbsp; = &nbsp; &Theta;(<i>n</i><sup>3</sup>) /
&Theta;(<i>n</i>) &nbsp; = &nbsp; &Theta;(<i>n</i><sup>2</sup>)</p>

<p><i><u>Exercise</u>: Could we get the span down to &Theta;(1) if we
parallelized the inner <b>for</b> with <b>parallel for</b>? You should be able
to answer this based on the previous exercise.</i></p>

<h4>Multithreading a divide and conquer algorithm</h4>

<p>Here is a parallel version of the divide and conquer algorithm from Chapter 4
of CLRS (not in these web notes):</p>

<img src="Topic-22/code-P-Matrix-Multiply-Recursive.jpg">

<p>See the text for analysis, which concludes that while the work is still
&Theta;(<i>n</i><sup>3</sup>), the span is reduced to
&Theta;(lg<sup>2</sup><i>n</i>). Thus, while the work is the same as the basic
algorithm the parallelism is &Theta;(<i>n</i><sup>3</sup>) /
&Theta;(lg<sup>2</sup><i>n</i>), which makes good use of parallel resources.</p>

<!-- ------------------------------------------------------------ -->
<hr><h2> Example: Merge Sort </h2>

<p>Divide and conquer algorithms are good candidates for parallelism, because
they break the problem into independent subproblems that can be solved
separately. We look briefly at merge sort.</p>

<h4>Parallelizing Merge-Sort</h4> 

<p>The dividing is in the main procedure <tt>MERGE-SORT</tt>, and we can
parallelize it by spawning the first recursive call:</p>

<img src="Topic-22/code-Merge-Sort-Prime.jpg">

<p><tt>MERGE</tt> remains a serial algorithm, so its work and span are
&Theta;(<i>n</i>) as before. </p>

<p>The recurrence for the <i>work</i> MS'<sub>1</sub>(<i>n</i>) of
<tt>MERGE-SORT'</tt> is the same as the serial version:</p>

<img src="Topic-22/equation-Merge-Sort-Prime-work.jpg">

<p>The recurrence for the <i>span</i> MS'<sub>&infin;</sub>(<i>n</i>) of
<tt>MERGE-SORT'</tt> is based on the fact that the recursive calls run in
parallel, so there is only one <i>n</i>/2 term (they are the same, so <i>min</i>
takes either):</p>

<img src="Topic-22/equation-Merge-Sort-Prime-span.jpg">

<p>The <i>parallelism</i> is thus MS'<sub>1</sub>(<i>n</i>) /
MS'<sub>&infin;</sub>(<i>n</i>) &nbsp; = &nbsp; &Theta;(<i>n</i> lg <i>n</i> /
<i>n</i>) &nbsp; = &nbsp; &Theta;(lg <i>n</i>). </p>

<p>This is low parallelism, meaning that even for large input we would not
benefit from having hundreds of processors. How about speeding up the serial
<tt>MERGE</tt>? </p>

<h4>Parallelizing Merge</h4>

<p><tt>MERGE</tt> takes two sorted lists and steps through them together to
construct a single sorted list. This seems intrinsically serial, but there is a
clever way to make it parallel.</p>

<p> A divide-and-conquer strategy can rely on the fact that the lists are sorted
to break the lists into four lists, two of which will be merged to form the head
of the final list and the other two merged to form the tail.</p>

<p>To find the four lists for which this works, we 
<ol>
  
  <li>Choose the longer list to be the first list, T[<i>p</i><sub>1</sub>
      .. <i>r</i><sub>1</sub>] in the figure below.</li>
  
  <li>Find the middle element (median) of the first list (<i>x</i> at
      <i>q</i><sub>1</sub>).</li>
  
  <li>Use binary search to find the position (<i>q</i><sub>2</sub>) of this
      element if it were to be inserted in the second list
      T[<i>p</i><sub>2</sub> .. <i>r</i><sub>2</sub>].</li>
  
  <li>Recursively merge 
      <ul>
        
        <li>The first list up to just before the median T[<i>p</i><sub>1</sub>
             ..  <i>q</i><sub>1</sub>-1] and the second list up to the insertion
             point T[<i>p</i><sub>2</sub> .. <i>q</i><sub>2</sub>-1].</li>
        
        <li>The first list from just after the median T[<i>q</i><sub>1</sub>+1
            ..  <i>r</i><sub>1</sub>] and the second list after the insertion
            point T[<i>q</i><sub>2</sub> .. <i>r</i><sub>2</sub>].</li>
        
      </ul>
    </li>
    
  <li>Assemble the results with the median element placed between them, as shown
    below.</li>
    
</ol>

<img src="Topic-22/Fig-27-6-Multithreaded-Merge.jpg">

<p>The text presents the <tt>BINARY-SEARCH</tt> pseudocode and analysis of
&Theta;(lg <i>n</i>) worst case; this should be review for you. It then
assembles these ideas into a parallel merge procedure that merges into a second
array Z at location <i>p</i><sub>3</sub> (<i>r</i><sub>3</sub> is not provided
as it can be computed from the other parameters):</p>

<img src="Topic-22/code-P-Merge.jpg">

<h4>Analysis</h4> 

<p>My main purpose in showing this to you is to see that even apparently serial
algorithms sometimes have a parallel alternative, so we won't get into details,
but here is an outline of the analysis:</p>

<p>The span of <tt>P-MERGE</tt> is the maximum span of a parallel recursive
call. Notice that although we divide the first list in half, it could turn out
that <i>x</i>'s insertion point <i>q</i><sub>2</sub> is at the beginning or end
of the second list. Thus (informally), the maximum recursive span is 3<i>n</i>/4
(as at best we have "chopped off" 1/4 of the first list).</p>

<p>The text derives the recurrence shown below; it does not meet the Master
Theorem, so an approach from a prior exercise is used to solve it: </p> <img
src="Topic-22/equation-P-Merge-span.jpg">

<p>Given 1/4 &le; &alpha; &le; 3/4 for the unknown dividing of the second array,
the work recurrence turns out to be:</p>

<img src="Topic-22/equation-P-Merge-work.jpg">

<p>With some more work, PM<sub>1</sub>(<i>n</i>) = &Theta;(n) is derived. Thus
the parallelism is &Theta;(n / lg<sup>2</sup><i>n</i>)</p>

<p>Some adjustment to the <tt>MERGE-SORT'</tt> code is needed to use this
<tt>P-MERGE</tt>; see the text. Further analysis shows that the work for the new
sort, <tt>P-MERGE-SORT</tt>, is PMS<sub>1</sub>(<i>n</i> lg <i>n</i>) =
&Theta;(<i>n</i>), and the span PMS<sub>&infin;</sub>(<i>n</i>) =
&Theta;(lg<sup>3</sup><i>n</i>). This gives parallelism of &Theta;(<i>n</i> /
lg<sup>2</sup><i>n</i>), which is much better than &Theta;(lg <i>n</i>) in terms
of the potential use of additional processors as <i>n</i> grows.</p>

<p>The chapter ends with a comment on coarsening the parallelism by using an
ordinary serial sort once the lists get small. One might consider whether
<tt>P-MERGE-SORT</tt> is still a stable sort, and choose the serial sort to
retain this property if it is desirable.</p>

<!-- --------------------------------------------------------------------- -->
<hr> <h2>Scheduling</h2>

<p>At the beginning, we noted that we rely on a concurrency platform to
determine how to allocate potentially parallel threads of computation to
available processors. This is the <b>scheduling problem</b>. Scheduling parallel
computations is a complex problem, and sophisticated schedulers have been
designed that are beyond what we can discuss here. </p>

<p><b>Centralized schedulers</b> are those that have information on the global
state of computation, but must make decisions in real time rather than in
batch. A simple approach to centralized scheduling is a <b> greedy
scheduler</b>, which assigns as many strands to available processors as possible
at any given time step. The CLRS texts proves a theorem concerning the
performance of a greedy scheduler, with interesting corollaries:</p>

<blockquote>
  <i>Theorem:</i> On an ideal parallel computer with <i>P</i> processors, a
  greedy scheduler executes a multithreaded computation with work
  <i>T</i><sub>1</sub> and span <i>T</i><sub>&infin;</sub> in time
  <i>T</i><sub><i>P</i></sub> &le; <i>T</i><sub>1</sub> +
  <i>T</i><sub>&infin;</sub>.
</blockquote>

<blockquote>
  <i>Corollary:</i> The running time <i>T</i><sub><i>P</i></sub> of any
  multithreaded computation scheduled by a greedy scheduler on an ideal parallel
  computer with <i>P</i> processors is within a factor of 2 of optimal.
</blockquote>

<blockquote>
  <i>Corollary:</i> As slackness grows a greedy scheduler achieves near-perfect
  linear speedup on any multithreaded computation.
</blockquote>

<p>The proofs are not difficult to understand: see the text if you are
interested. I think we have said enough here to introduce the concepts of
multithreading.</p>

<hr> <h2> Final Note</h2>

<p>Professor Henri Casanova does research on scheduling, and Professor Nodari
Sitchinava does research on parallel algorithms. They would be happy to talk to
interested students.</p>


<!-- ------------------------------------------------------------ -->
<hr>
<address>Dan Suthers</address>
<!-- hhmts start -->Last modified: Sun Nov 29 04:51:33 HST 2020 <!-- hhmts end -->
<br>Images are from the instructor's material for Cormen et al. Introduction to
Algorithms, Third Edition.</br>
</body>
</html>
