<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title>ICS 311 #15: Amortized Analysis </title>
</head>

<body>

<hr><h1><a href="../index.html">ICS 311</a> #15: Amortized Analysis </h1><hr> 
<!-- Spring 2014 version -->

<!-- ------------------------------------------------------------ -->
<h2>Outline</h2>

<ol>
  <li>Amortized Analysis: The General Idea </li>
  <li>Multipop Example </li>
  <li>Aggregate Analysis </li>
  <li>Accounting Method</li>
  <li>Potential Method - <i> Optional for 2020, but it's the most powerful
      method </i> </li> 
  <li>Dynamic Table Example (first look)</li> 
  <li>Other Examples</li>
</ol>

<h2>Readings and Screencasts</h2>
<ul>
  <li>Chapter 17 of Cormen et al. (2015: through 17.2, plus 17.4.1) </li>  
  <li>Screencasts <a href="http://youtu.be/rlIka0W814U">15A</a>, 
       <a href="http://youtu.be/iy-WhloN6vA">15B</a> - 
       (also in Laulima)</li>
</ul>


<!-- ------------------------------------------------------------ -->
<hr><h2> Amortized Analysis: The General Idea </h2>

<p>We have already used <em>aggregate</em> analysis several times in this
course. For example, when analyzing the BFS and DFS procedures, instead of
trying to figure out how many times their inner loops</p>

<blockquote> 
<tt>for each <i>v</i> &in; G.Adj[<i>u</i>]</tt> 
</blockquote> 

<p> execute (which depends on the degree of the vertex being processed), we
realized that no matter how the edges are distributed, there are at most
|<i>E</i>| edges, so in aggregate across all calls the loops will execute
|<i>E</i>| times.</p>

<p>But that analysis concerned itself only with the complexity of a single
operation. In practice a given data structure will have associated with it
several operations, and they may be applied many times with varying
frequency.</p>

<p>Sometimes a given operation is designed to pay a larger cost than would
otherwise be necessary to enable other operations to be lower cost.</p>

<p><i>Example:</i> Red-black tree insertion. We pay a greater cost for balancing
so that future searches will remain O(lg <i>n</i>).</p>

<p><i>Another example:</i> Java Hashtable.</p>
<ul>
  <li>These grow dynamically when a specified load factor is exceeded.</li> 
  <li> Copying into a new table is expensive, but copying is infrequent and
       table growth makes access operations faster.</li>
</ul> 

<p>It is "fairer" to average the cost of the more expensive operations across
the entire mix of operations, as all operations benefit from this cost.</p>

<p>Here, "average" means average cost in the worst case (no probability is
involved, which greatly simplifies the analysis).</p>

<p>We will look at three methods. The notes below use Stacks with Multipop to
illustrate the methods. See the text for binary counter examples.</p>

<p>(We have already seen examples of aggregate analysis throughout the
semseter. We will see examples of amortized analysis later in the semester.)</p>

<!-- ------------------------------------------------------------ -->
<hr><h2> Multipop Example </h2>

<p>We already have the stack operations:</p>
<ul>

  <li><tt>Push(<i>S, x</i>)</tt>: O(1) each call, so O(<i>n</i>) for any
      sequence of <i>n</i> operations.</li>

  <li><tt>Pop(<i>S</i>)</tt>: O(1) each call, so O(<i>n</i>) for any sequence of
      <i>n</i> operations.</li>

</ul> 

<p>Suppose we add a <tt>Multipop</tt> (this is a generalization of
<tt>ClearStack</tt>, which empties the stack): </p>

<img src="Topic-15/Fig-17-1-multipop.jpg" align="right" border="1">
<img src="Topic-15/pseudocode-multipop.jpg">

<p>The example shows a <tt>Multipop(S,4)</tt> followed by another where <i>k</i>
&ge; 2.</p>

<p>Running time of <tt>Multipop</tt>: 
<ul>

  <li><tt>Pop</tt> operations are O(1) (one per loop iteration)</li>

  <li>Number of iterations of <tt>while</tt> loop is min(<i>s</i>, <i>k</i>),
      where <i>s</i> = number of items on the stack</li>

  <li>Therefore, total cost is proportional to min(<i>s</i>, <i>k</i>). </li> 

</ul>

<p>What is the worst case of a sequence of <i>n</i> <tt>Push</tt>, <tt>Pop</tt>
and <tt>Multipop</tt> operations?</p>

<p>Using our existing methods of analysis we can identify a <i>loose bound</i>
over <i>n</i> operations:

<ul>
  <li> The most expensive operation is <tt>Multipop</tt>, potentially
       O(<i>n</i>).</li>
  <li> Therefore, potentially O(<i>n<sup>2</sup></i>) over
       <i>n</i> operations.</li>
</ul>

<p><i>Can you see a flaw in this reasoning?</i> </p> 

<!-- ------------------------------------------------------------ -->
<hr><h2> <a name="aggregate"> Aggregate Analysis </a> </h2>

<p>We can tighten this loose bound by aggregating the analysis over all <i>n</i>
operations:</p>

<ul>
 <li>Each object can only be popped once per time that it is pushed.</li> 
 <li>There are at most <i>n</i> <tt>Push</tt>es, so at most n <tt>Pop</tt>s,
     including those in <tt>Multipop</tt></li> 
 <li>Therefore, total cost = O(<i>n</i>)</li>
 <li>Averaging over the <i>n</i> operations we get O(1) per operation.</li>
</ul>

<p>This analysis shows O(1) per operation on average in a sequence of <i>n</i>
operations without using any probabilities of operations.</p>

<p>See the CLRS text for an example of aggregate analysis of binary counting. An
example of aggregate analysis of dynamic tables is at the end of these
notes.</p>

<p>Some of our previous analyses with indicator random variables have been a
form of aggregate analysis, specifically our analysis of the expected number of
inversions in sorting, <a href="Topic-05.html#inversions">Topic 5</a>, and our
analysis of Depth-First Search, <a href="Topic-14.html#DFStime">Topic
14</a>.</p>

<p>Aggregate analysis treats all operations equally. The next two methods let us
give different operations different costs, so are more flexible. </p>

<!-- ------------------------------------------------------------ -->
<hr><h2> Accounting Method </h2>

<h4>Consider this Metaphor:</h4> 
<ul>

  <li>View the computer as a coin operated appliance that requires one
      <i>cyber-dollar</i> (CY$) per basic operation.</li>

  <li>When an operation is to be performed we must have enough cyber-dollars
      available to pay for it (no "loans" allowed).  </li>

  <li>We are permitted to charge some operations more than they actually cost so
      we can save enough to pay for the more expensive operations.</li>

</ul>

<p>The <b>amortized cost</b> is the amount we charge each operation, which may
be different from the operation actually costs.</p>

<p>This differs from aggregate analysis:</p>
<ul>
  <li>In aggregate analysis, all operations have the same cost.</li>
  <li>In the accounting method, different operations can have different
      costs.</li> 
</ul> 

<p>When an operation is overcharged (amortized cost &gt; actual cost), the
difference is associated with <i>specific objects</i> in the data structure as
<i>credit</i>. </p>

<p>We use this credit to pay for operations whose actual cost &gt; amortized
cost.</p>

<p>The credit must never be negative. Otherwise the amortized cost may not be an
upper bound on actual cost for some sequences.</p>

<p>Let </p>
<ul>
  <li><i>c<sub>i</sub></i> = actual cost of <i>i</i>th operation. </li> 
  <li><i>&ccirc;<sub>i</sub></i> = amortized cost of <i>i</i>th operation
      <i>(notice the 'hat': it is pronounced "c-hat")</i>. </li> 
</ul> 

<p>Require &sum;<sub><i>i</i>=1,<i>n</i></sub><i> &ccirc;<sub>i</sub></i> &nbsp;
&ge; &nbsp; &sum;<sub><i>i</i>=1,<i>n</i></sub> <i>c<sub>i</sub></i> for all
sequences of <i>n</i> operations. That is, the difference between these sums is
always &ge; 0: we never owe anyone anything, and &ccirc; will provide an upper
bound on the actual cost for all sequences.</p>

<!-- --------------- -->
<h3>Stack Example</h3>

<p>Whenever we <tt>Push</tt> an object (at actual cost of 1 cyberdollar), we
potentially have to pay CY$1 in the future to <tt>Pop</tt> it, whether directly
or in <tt>Multipop</tt>.</p>

<p>To make sure we have enough money to pay for the <tt>Pops</tt>, we charge
<tt>Push</tt> CY$2. </p>

<ul>
  <li>CY$1 pays for the push</li>
  <li>CY$1 is prepayment for the object being popped (metaphorically, this CY$1
  is stored "on" the object).</li> 
</ul> 

<p>Since each object has CY$1 credit, the credit can never go negative.</p>

<img src="Topic-15/stack-amortized-cost-table.jpg">

<p>The total amortized cost <i>&ccirc;</i> = &sum;<sub><i>i</i>=1,<i>n</i></sub>
<i>&ccirc;<sub>i</sub></i> for <em>any</em> sequence of <i>n</i> operations is
an upper bound on the total actual cost <i>c</i> =
&sum;<sub><i>i</i>=1,<i>n</i></sub> <i>c<sub>i</sub></i> for that sequence. This
is clearly true in the case of PUSH operations where the amortized cost $2 is
greater than the actual cost $1. It is true for the other operations because
they can only incur an actual cost of $1 per item when operating on items in the
stack, and there will be $1 credit associated with each item. </p>

<p>Since <i>&ccirc;</i> = O(<i>n</i>), also <i>c</i> = O(<i>n</i>). </p> 

<p>Note: we don't actually store cyberdollars in any data structures. This is
just a metaphor to enable us to compute an amortized upper bound on costs.</p>

<p>Does it seem strange that we charge <tt>Pop</tt> and <tt>Multipop</tt> 0 when
we know they cost something? </p>

<ul>
  <li> Remember that this is just a way of counting the total cost over a
       sequence of operations more precisely.</li>
  <li> It is not a claim about the actual cost of a specific procedural
       call.</li> 
  <li> We are guaranteeing that we have just enough credit on hand to pay for
       the operations when they happen.</li> 
  <li> The accounting method and the potential method below give a tight bound
       on amortized cost, but with much easier counting than if we had to reason
       about probability distributions, etc.</li>  
</ul> 

<!-- ------------------------------------------------------------ -->
<hr><h2> Potential Method </h2>

<h4> (This method is optional this semester: you may skip to Application:
Dynamic Tables.)</h4>

<p>Instead of credit associated with objects in the data structure, this method
uses the metaphor of <i>potential</i> associated with the <i>entire data
structure.</i></p>

<p>(I like to think of this as potential <i>energy,</i> but the text continues
to use the monetary metaphor.)</p>

<p>This is the most flexible of the amortized analysis methods, as it does not
require maintaining an object-to-credit correspondence.</p>

<p>Let </p>
<ul>
  <li>D<sub>0</sub> = initial data structure </li>
  <li>D<sub><i>i</i></sub> = data structure after <i>i</i>th operation</li>
  <li><i>c<sub>i</sub></i> = actual cost of <i>i</i>th operation. </li>
  <li><i>&ccirc;<sub>i</sub></i> = amortized cost of <i>i</i>th operation
      <i>(again, notice that this is "c-hat")</i>. </li>
</ul> 

<p>The <b>Potential Function &Phi;</b>: D<sub><i>i</i></sub> &rarr; &real; maps
states to real numbers (the potential). We say that &Phi;(D<sub><i>i</i></sub>)
is the <b>potential</b> associated with the data structure in state
D<sub><i>i</i></sub>. </p>

<p>We define the amortized cost <i>&ccirc;<sub>i</sub></i> to be the actual cost
<i>c<sub>i</sub></i> plus the change in potential due to the <i>i</i>th
operation:</p> <blockquote> <i>&ccirc;<sub>i</sub></i> = <i>c<sub>i</sub></i> +
&Phi;(D<sub><i>i</i></sub>) &minus; &Phi;(D<sub><i>i-1</i></sub>) </blockquote>

<ul>

  <li>If at the <i>i</i>th operation, &Phi;(D<sub><i>i</i></sub>) &minus;
      &Phi;(D<sub><i>i-1</i></sub>) is positive, then the amortized cost
      <i>&ccirc;<sub>i</sub></i> is an <i>overcharge</i> and the potential of
      the data structure increases.</li>


<li>On the other hand, if &Phi;(D<sub><i>i</i></sub>) &minus;
    &Phi;(D<sub><i>i-1</i></sub>) is negative then <i>&ccirc;<sub>i</sub></i> is
    an undercharge, and the decrease of the potential of the data structure pays
    for the difference (as long as it does not go negative). </li>

</ul>

<p>The total amortized cost across <i>n</i> operations is:</p>
<blockquote>
  &sum;<sub><i>i</i>=1,<i>n</i></sub> <i>&ccirc;<sub>i</sub></i> &nbsp; = &nbsp;
  &sum;<sub><i>i</i>=1,<i>n</i></sub>(<i>c<sub>i</sub></i> +
  &Phi;(D<sub><i>i</i></sub>) - &Phi;(D<sub><i>i-1</i></sub>)) &nbsp; = &nbsp;
  (&sum;<sub><i>i</i>=1,<i>n</i></sub> <i>c<sub>i</sub></i>) +
  (&Phi;(D<sub><i>n</i></sub>) - &Phi;(D<sub>0</sub>))
</blockquote>

<p>The last step is taken because the middle expression involves a
<u>telescoping sum</u>: every term other than D<sub><i>n</i></sub> and
D<sub>0</sub> is added once and subtracted once.</p>

<p>If we require that &Phi;(D<sub><i>i</i></sub>) &ge; &Phi;(D<sub>0</sub>) for
all <i>i</i> then the amortized cost will always be an upper bound on the actual
cost no matter which <i>i</i>th step we are on. </p>

<p>This is usually accomplished by defining &Phi;(D<sub>0</sub>) = 0 and then
showing that &Phi;(D<sub><i>i</i></sub>) &ge; 0 for all <i>i</i>. (Note that
this is a constraint on &Phi;, not on <i>&ccirc;</i>. <i>&ccirc;</i> can go
negative as long as &Phi;(D<sub><i>i</i></sub>) never does.)</p>

<!-- --------------- -->
<h3>Stack Example</h3>

<p>Define &Phi;(D<sub><i>i</i></sub>) = number of objects in the stack.</p>

<p>Then &Phi;(D<sub>0</sub>) = 0 and &Phi;(D<sub><i>i</i></sub>) &ge; 0 for all
<i>i</i>, since there are never less than 0 objects on the stack.</p>

<p>Charge as follows (recalling that <i>&ccirc;<sub>i</sub></i> =
<i>c<sub>i</sub></i> + &Phi;(D<sub><i>i</i></sub>) -
&Phi;(D<sub><i>i-1</i></sub>), the "actual cost" column is <i>c<sub>i</sub></i>,
and the '&Delta;&Phi;" column computes the difference
&Phi;(D<sub><i>i</i></sub>) - &Phi;(D<sub><i>i-1</i></sub>)):</p>

<img src="Topic-15/stack-potential-table.jpg">

<p>Since we charge 2 for each <tt>Push</tt> and there are O(n) Pushes in the
worst case, the amortized cost of a sequence of <i>n</i> operations is
O(<i>n</i>). </p>

<!-- ------------------------------------------------------------ -->
<hr><h2>Application: Dynamic Tables</h2>

<p>There is often a tradeoff between time and space, for example, with hash
tables. Bigger tables give faster access but take more space. </p>

<p>Dynamic tables, like the Java Hashtable, grow dynamically as needed to keep
the load factor reasonable. </p>

<p>Reallocation of a table and copying of all elements from the old to the new
table is expensive!</p>

<p>But this cost is amortized over all the table access costs in a manner
analogous to the stack example: We arrange matters such that table-filling
operations build up sufficient credit before we pay the large cost of copying
the table; so the latter cost is averaged over many operations.</p>

<!-- ----------------------------- -->
<h3>A Familiar Definition</h3>

<p><b>Load factor &alpha;</b> = <i>num</i>/<i>size</i>, where <i>num</i> = #
items stored and <i>size</i> = the allocated size of the table.</p>

<p>For the boundary condition of <i>size</i> = <i>num</i> = 0, we will define
&alpha; = 1.</p>

<p>We never allow &alpha; &gt; 1 (no chaining).</p>

<!-- ----------------------------- -->
<h3>Insertion Algorithm</h3>

<p>We'll assume the following about our tables. (See Exercises 17.4-1 and 17.4-3
concerning different assumptions.):</p>

<p>When the table becomes full, we double its size and reinsert all existing
items. This guarantees that &alpha; &ge; 1/2, so we are not wasting a lot of
space. </p>

<pre>
  Table-Insert (T,x)
    1  if T.size == 0
    2      allocate T.table with 1 slot 
    3      T.size = 1
    4  if T.num == T.size
    5      allocate newTable with 2*T.size slots
    6      insert all items in T.table into newTable
    7      free T.table
    8      T.table = newTable 
    9      T.size = 2*T.size 
   10  insert x into T.table 
   11  T.num = T.num + 1
</pre> 

<p>Each <i>elementary insertion</i> <tt>insert x into T.table</tt> has unit
actual cost. Initially <i>T.num</i> = <i>T.size</i> = 0.</p>

<!-- ----------------------------------- -->
<h3>Aggregate Analysis of Dynamic Table Expansion</h3> 

<p>Charge 1 per elementary insertion (including copying items into a new
table). Count only these insertions, since all other costs are constant per
call. </p>

<p><b><i>c<sub>i</sub></i></b> = actual cost of <i>i</i>th operation (number of
items inserted).</p>


<ul>
  <li> If the table is not full, <i>c<sub>i</sub></i> = 1 (for lines 1, 4, 10,
       11). </li> 
  <li> If full, there are <i>i</i> - 1 items in the table at the start of the
       <i>i</i>th operation. Must copy all of them (line 6), and then insert the
       <i>i</i>th item. Therefore  
       <i>c<sub>i</sub></i> = <i>i</i> - 1 + 1 = <i>i</i>.</li>
</ul>

<p>A sloppy analysis: In a sequence of <i>n</i> operations where any operation
can be O(<i>n</i>), the sequence of <i>n</i> operations is
O(<i>n</i><sup>2</sup>). </p>

<p>This is "correct", but inprecise: we rarely expand the table! A more precise
account of <i>c<sub>i</sub></i> is based on the fact that if we start with a
table of size 1 and double its size every time we expand it, we do the
expansions when the number of items in the table is a power of 2:</p>

<img src="Topic-15/c_i-definition.jpg">

<p>Then we can sum the total cost of all <i>c<sub>i</sub></i> for a sequence of
<i>n</i> operations:</p>

<img src="Topic-15/analysis-table-expansion.jpg">
<img src="Topic-15/formula-A-5.jpg" align="right" border="1">

<p><em>Explanation of steps:</em>
<ul>

  <li> Why is <i>n</i> +... added in the second line?<br> &nbsp; &nbsp;  &rarr; It
       counts the "1" for "otherwise". It counts too many of them; hence the
       &le;. </li>

  <li> What is the summation term in the second line counting? .... <br> &nbsp;
        &nbsp; &rarr; The costs for copying at exact powers of 2. </li>

  <li> Why does the summation start at <i>j</i> = 0? Why does it end at <i>j</i>
       = lg <i>n</i>?  ...<br> &nbsp; &nbsp; &rarr; You can explain this! look
       at the values of 2<sup><i>j</i></sup>.</li>

  <li> How did we get rid of the summation? ...<br> &nbsp; &nbsp; &rarr; See the
         formula in the box. </li>

  <li> How did we get 2<i>n</i>? ...<br> &nbsp; &nbsp; &rarr;
         2<sup><i>x</i>+1</sup> = 2*2<sup><i>x</i></sup>, and here <i>x</i> = lg
         <i>n</i>, so 2 to that power is by definition <i>n</i>. <br> &nbsp;
         &nbsp The &minus;1 was removed by using &lt; and the denominator is
         simply 1.</li>

</ul> 

<p>Therefore, the amortized cost per operation = 3: we are only paying a small
constant price for the expanding table.</p>

<!-- ------------------------------------ -->
<h3>Accounting Method Analysis of Dynamic Table Expansion</h3> 

<p>We charge CY$3 for each insertion into the table, i.e., the amortized cost of
the <i>i</i>-th insertion as <i>&ccirc;<sub>i</sub></i> = 3. The actual cost of
the <i>i</i>-th insertion is as before:</p>

<img src="Topic-15/c_i-definition.jpg">

<p> We must show that we never run out of credit. It's equivalent to proving the
following Lemma.</p>

<p><b>Lemma: <i>For any sequences of n insertions</i>
&sum;<sub><i>i</i>=1,<i>n</i></sub> <i>&ccirc;<sub>i</sub></i> &nbsp; &ge;
&nbsp; &sum;<sub><i>i</i>=1,<i>n</i></sub> <i>c<sub>i</sub></i>.</b> </p>

<p> <i>Proof</i>: By using the same equation as above, we know that for any
<i>n</i>: &sum;<sub><i>i</i>=1,<i>n</i></sub> <i>c<sub>i</sub></i> &le;
3<i>n</i>. By the definition of &ccirc;, we know that:
&sum;<sub><i>i</i>=1,<i>n</i></sub> <i>&ccirc;<sub>i</sub></i> = 3<i>n</i>. The
lemma follows. &#x220E;</p>

<p>That's it!</p>

<!-- ------------------------------------ -->
<h3>Informal Explanation </h3>

<p>In case it helps to understand why this works:</p>

<p>We charge CY$3 for inserting some item <i>x</i> into the table. Obviously, we
are overcharging for each <i>simple</i> insertion which costs only
CY$1. However, the overcharging will provide enough credit to pay for future
copying of items when the table becomes full:

<ul>
  <li>The first CY$1 pays for the actual cost of inserting <i>x</i>.</li>  
  <li>The second CY$1 will pay for the cost of copying <i>x</i> into a new table
      the next time table becomes full. </li> 
</ul>

<p> But the table might need to be expanded more than once after <i>x</i> is
inserted, so <i>x</i> might need to be copied more than once.  Who will pay for
future copying of <i>x</i>? That's where the third CY$1 comes in: </p>

<ul>
  <li>The third CY$1 will pay for the cost of copying some other item currently
      in the table that had been copied at least once before. </li>  
</ul>


<p> Let's see why CY$3 is enough to cover all possible future exansions and
copying associated with them. </p>

<ul>

  <li> Suppose the capacity of the table is <i>m</i> immediately after an
       expansion. Then it holds <i>m</i>/2 items and no item in the table
       contains any credits. </li>

  <li> For any insertion of an item <i>x</i> we charge CY$3. CY$1 pays for the
       actual insertion of <i>x</i>; we place CY$1 credit on <i>x</i> to pay for
       the cost of copying it in the future, and we place CY$1 credit on some
       other item in the table that does not have any credit yet.</li>

  <li> We will have to insert <i>m</i>/2 items before the next expansion of the
       table. Therefore, by the time the table will get expanded next time (and,
       consequently, items need to be copied), every item of the table will have
       CY$1 credit associated with it and this credit will pay for copying that
       item.</li>
       
</ul>

<p> The text also provides the potential analysis of table expansion.</p>

<p>This analysis assumed that the table never shrinks. See section 17.4 <!--(and
your homework) --> for an analysis using the potential method that covers
shrinking tables.</p>


<!-- ------------------------------------------------------------ -->
<hr><h2>Other Examples</h2>

<p>Here are some other algorithms for which amortized analysis is useful:</p> 
<!-- --------------- -->
<h3>Red-Black Trees</h3>

<p>An amortized analysis of Red-Black Tree restructuring (Problem 17-4 of CLRS)
improves upon our analysis earlier in the semester:</p>

<ul>

  <li> Any sequence of <i>m</i> <tt>RB-Insert</tt> and <tt>RB-Delete</tt>
       operations performs O(<i>m</i>) structural modifications (rotations),
       </li>

  <li> Thus each operation does <b>O(1) structural modifications on average</b>,
       regardless of the size of the tree!</li>

  <li> An operation still may need to do O(lg <i>n</i>) recolorings, but these
       are very simple operations.</li>

</ul>

<!-- --------------- -->
<h3>Self-Organizing Lists</h3>

<ul> 

  <li> Self-organizing lists use a <b><i>move-to-front heuristic</i></b>:
       Immediately after searching for an element, it is moved to the front of
       the list.</li>

  <li>This makes frequently accessed items more readily available near the front
       of the list.</li>

  <li> An amortized analysis (Problem 17-5) shows that the heuristic is no
       more than 4 times worse than optimal.</li>

</ul>

<!-- --------------- -->
<h3>Splay Trees</h3> 

<ul>

  <li> Splay trees are ordinary binary search trees (no colors, no height
       labels, etc.)</li>

  <li> After every access (every insertion, deletion, or search), the element
       operated on (or its parent in the case of deletion) is moved towards the
       top of the tree.</li>

  <li> This movement uses three <b><i>splaying</i></b> operations called "zig",
       "zig-zig" and "zig-zag".</li>

  <li> Although in the worst case a splay tree can degenerate into an
       O(<i>n</i>) linked list, amortized analysis shows that the expected case
       is O(lg <i>n</i>)</li>

  <li> Randomization can be used to make the worst case very unlikely.</li>

  <li> If a single element is accessed at least <i>m</i>/4 times where <i>m</i>
       is the number of operations, then the amortized running time of each of
       these accesses is O(1).</li>

  <li> Thus, splay-trees self-organize to provide fast access to frequently
       accessed items.</li>

  <li> This makes them good for locality of reference in memory, but
       multithreaded access must be implemented carefully.</li>

</ul> 

<!-- --------------- -->
<h3>To Be Continued </h3>

<p>Amortized analysis will be used in analyses of</p>

<ul>
  <li>Graph search (Topic 14, Ch. 22) </li>
  <li>Disjoint set operations (Topic 16, Ch. 21) </li>
  <li>Dijkstra's Algorithm for Shortest Paths (Topic 18, Ch. 24) </li> 
</ul>

<!-- ------------------------------------------------------------ -->
<hr>
<address>Dan Suthers &amp; Nodari Sitchinava</address>
<!-- hhmts start -->Last modified: Sun Nov  1 02:50:15 HST 2020 <!-- hhmts end -->
<br>Images are from the instructor's material for Cormen et al. Introduction to Algorithms, Third
Edition.</br> 
</body>
</html>
